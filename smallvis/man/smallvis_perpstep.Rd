% Generated by roxygen2: do not edit by hand
% Please edit documentation in R/iterated.R
\name{smallvis_perpstep}
\alias{smallvis_perpstep}
\title{Dimensionality Reduction With Perplexity Stepping}
\usage{
smallvis_perpstep(step_iter = 250, ...)
}
\arguments{
\item{step_iter}{Number of iterations to carry out the perplexity
stepping. Must be < the value of \code{max_iter}.}

\item{...}{Arguments to be passed to \code{\link{smallvis}}. See 'Details'
for information on which arguments may be modified or ignored during certain
parts of the embedding.}
}
\value{
The result of the final run of \code{\link{smallvis}} at the target
  perplexity.
}
\description{
Carry out dimensionality reduction of a (small) dataset using one of a
variety of neighbor embedding methods, using a decreasing value of
perplexity to avoid bad local minima.
}
\details{
This function uses ideas similar to those in the NeRV (Venna et al., 2010)
and JSE (Lee et al., 2013), where to avoid local minima, the initial
optimization steps use affinities with larger bandwidths (NeRV) or larger
perplexity values (JSE). This implementation uses a series of decreasing
perplexity values, as in JSE.

For details on the arguments that can be passed to the dimensionality
reduction routine, see the help text for \code{\link{smallvis}}.

To avoid spending too much extra time in perplexity calibrations, the extra
perplexities start at the power of 2 closest to, but not greater than,
half the dataset size (in terms of number of objects). Further calibrations
are then carried out halving the perplexity each time, until the perplexity
specified by the user is reached.

The number of iterations spent in the larger perplexity values is specified
by the \code{step_iter} parameter. This determines the total number
of iterations, e.g. if \code{step_iter = 250} and extra optimizations
at a perplexity of 1024, 512, 256, 128 and 64 will be carried out, these will
run for 50 iterations each. To keep the number of iterations equivalent to
that used by a single run of \code{\link{smallvis}}, the value of
\code{step_iter} is subtracted from the value of \code{max_iter} before
the optimization at the target perplexity is carried out, e.g. if
\code{max_iter = 1000} and \code{step_iter = 250}, the final
optimization will run for 750 iterations only.

Any value of \code{tol}, \code{exaggeration_factor} and
\code{stop_lying_iter} provided is used only with the final optimization.
}
\examples{
\dontrun{
# t-SNE on the iris with L-BFGS optimization
# The 1000 max_iter is split between 250 iterations at perplexity = 64
# and then 750 iterations at perplexity = 40.
iris_lbfgs_pstep <- smallvis_perpscale(
  step_iter = 250, X = iris, scale = FALSE, verbose = TRUE, Y_init = "spca",
  ret_extra = c("DX", "DY"), perplexity = 40, max_iter = 1000, opt = list("l-bfgs")
)
}
}
\references{
Venna, J., Peltonen, J., Nybo, K., Aidos, H., & Kaski, S. (2010).
Information retrieval perspective to nonlinear dimensionality reduction for
data visualization.
\emph{Journal of Machine Learning Research}, \emph{11}, 451-490.

Lee, J. A., Renard, E., Bernard, G., Dupont, P., & Verleysen, M. (2013).
Type 1 and 2 mixtures of Kullback-Leibler divergences as cost functions in
dimensionality reduction based on similarity preservation.
\emph{Neurocomputing}, \emph{112}, 92-108.
}
